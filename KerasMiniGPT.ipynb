{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with a Miniature GPT (using Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Link to the tutorial given below:* \\\n",
    "[https://keras.io/examples/generative/text_generation_with_miniature_gpt/](https://keras.io/examples/generative/text_generation_with_miniature_gpt/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime Environment (Colab)\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch: 2.6.0+cu124 \\\n",
    "GPU: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "'''\n",
    "Forcing Keras to user Tensorflow as its backend (instead of\n",
    "alternatives like JAX or PyTorch).\n",
    "This in turn ensures compatibility with TensorFlow-specific features\n",
    "(eg; tensorflow.data etc)\n",
    "'''\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "'''\n",
    "keras: high level neural network API\n",
    "layers: this contains prebuild layers (eg; Dense, LSTM, Embedding)\n",
    "ops: provides low level operations (similar to NumPy but backend -\n",
    "agnostic)\n",
    "TextVectorisation: converts raw text into numerical tokens (text must\n",
    "be converted to integers for neural networks)\n",
    "'''\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "'''\n",
    "os: for filesystem operations (eg; loading datasets)\n",
    "string and random: for text preprocessing\n",
    "'''\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "\n",
    "'''\n",
    "tensorflow specific imports: core library for differentiable programming\n",
    "tf_data: tools for efficient data pipelines (eg; batching, shuffling)\n",
    "tf_strings: string manipulation operations\n",
    "'''\n",
    "import tensorflow\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a Transformer Block as a Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "creates a causal mask for autoregressive attention so that each token\n",
    "attends to only previous tokens (no 'peeking' into future)\n",
    "'''\n",
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "  '''\n",
    "  Mask the upper half of the dot product matrix in self attention,\n",
    "  This prevents flow of information from future tokens to current\n",
    "  tokens,\n",
    "  1's in the lower triangle, counting from the lower right corner\n",
    "  '''\n",
    "\n",
    "  '''\n",
    "  i = row indices (destination tokens)\n",
    "  j = column indices (source tokens)\n",
    "  m = boolean mask where i >=j (lower triangular + diagonal)\n",
    "  '''\n",
    "  i = ops.arange(n_dest)[:, None]\n",
    "  j = ops.arange(n_src)\n",
    "  m = i >= j - n_src + n_dest\n",
    "\n",
    "  '''\n",
    "  ops.cast: converts boolean mask to dtype (eg: float32 for softmax)\n",
    "  ops.tile: repeats the mask for all batches (batch_size copies)\n",
    "  '''\n",
    "  mask = ops.cast(m, dtype)\n",
    "  mask = ops.reshape(mask, [1, n_dest, n_src])\n",
    "  mult = ops.concatenate(\n",
    "      [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0\n",
    "  )\n",
    "  return ops.tile(mask, mult)\n",
    "\n",
    "'''\n",
    "making a single transformer decoder block from scratch (used in GPT)\n",
    "inherits: keras.layers.Layer (base class for custom layers)\n",
    "'''\n",
    "class TransformerBlock(layers.Layer):\n",
    "  def __init__(self, embed__dim, num_heads, ff_dim, rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    '''\n",
    "    components of Multi-Head Attention:\n",
    "    1. num_heads - parallel attention heads (eg; 8 heads)\n",
    "    2. embed_dim - token embedding dimension (eg; 512)\n",
    "    we use multi head attention because it captures diverse linguistic\n",
    "    patterns\n",
    "    '''\n",
    "    self.att = layers.MultiHeadAttention(num_heads, embed__dim)\n",
    "\n",
    "    '''\n",
    "    Feedforrward Network (ffn) -> this adds non linearity after\n",
    "    attention\n",
    "    Reference: Original Transformer uses ReLU (Attention is All you\n",
    "    need)\n",
    "    ff_dim -> hidden layer size (usually 4 * embed_dim)\n",
    "    '''\n",
    "    self.ffn = keras.Sequential([\n",
    "        layers.Dense(ff_dim, activation=\"relu\"),\n",
    "        layers.Dense(embed__dim),\n",
    "    ])\n",
    "\n",
    "    '''\n",
    "    LayerNorm - used primarily for normalization (stabilizes training)\n",
    "    Dropout - used for regularization (prevents overfitting)\n",
    "    '''\n",
    "    self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.dropout1 = layers.Dropout(rate)\n",
    "    self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "  def call(self, inputs):\n",
    "\n",
    "    input_shape = ops.shape(inputs)\n",
    "    batch_size = input_shape[0]\n",
    "    seq_len = input_shape[1]\n",
    "    causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, \"bool\")\n",
    "\n",
    "    attention_output = self.att(inputs, inputs, attention_mask = causal_mask)\n",
    "    attention_output = self.dropout1(attention_output)\n",
    "    out1 = self.layernorm1(inputs + attention_output)\n",
    "\n",
    "    ffn_output = self.ffn(out1)\n",
    "    ffn_output = self.dropout2(ffn_output)\n",
    "    return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement an Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenandPositionEmbedding(layers.Layer):\n",
    "  def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "    '''\n",
    "    Combines token embeddings and positional embeddings for transformer\n",
    "    inputs\n",
    "    - Token embeddings map discrete token IDs to continous vectors\n",
    "    - Positional embeddings encode sequential order\n",
    "    - Used in GPT and BERT\n",
    "    '''\n",
    "    super().__init__()\n",
    "    self.token_emb = layers.Embedding(\n",
    "        input_dim = vocab_size,\n",
    "        output_dim = embed_dim # Embedding dimension (eg; 512)\n",
    "    )\n",
    "    self.pos_emb = layers.Embedding(\n",
    "        input_dim = maxlen, # Maximum sequence length\n",
    "        output_dim = embed_dim\n",
    "    )\n",
    "\n",
    "  def call(self, x):\n",
    "    '''\n",
    "    Forward pass:\n",
    "    1. Generates positional indices for the input sequence,\n",
    "    2. Projects tokens and positions to the same embedding space,\n",
    "    3. Combines them additively\n",
    "    '''\n",
    "    maxlen = ops.shape(x)[-1]\n",
    "    positions = ops.arange(0, maxlen, 1) # [0, 1, ..., maxlen - 1]\n",
    "    positions = self.pos_emb(positions) # Positional embeddings\n",
    "    x = self.token_emb(x) # Token embeddings\n",
    "    return x + positions # Additive combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Miniature GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000 # Vocabulary size (top 20k words)\n",
    "maxlen = 80 # Maximum sequence length (tokens)\n",
    "embed_dim = 256 # Embedding dimension (d_model in \"Attention is All you need\")\n",
    "num_heads = 2 # Number of parallel attention heads\n",
    "feed_forward_dim = 256 # Hidden layer size in FFN (typically 4 * embed_dim in original Transformer)\n",
    "\n",
    "def create_model():\n",
    "  '''\n",
    "  Constructs a miniature GPT-like autoregressive language model\n",
    "  Architecture:\n",
    "  1. Token + Position embeddings\n",
    "  2. Transformer Decoder Block\n",
    "  3. Output Dense Layer (vocab_size logits)\n",
    "  '''\n",
    "\n",
    "  # Input layer (integer-encoded token sequences)\n",
    "  inputs = layers.Input(shape=(maxlen,), dtype=\"int32\")\n",
    "\n",
    "  # 1. Embedding Layer (Token + Position)\n",
    "  embedding_layer = TokenandPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "  x = embedding_layer(inputs)\n",
    "\n",
    "  # 2. Transformer Block (Autoregressive Decoder)\n",
    "  transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "  x = transformer_block(x)\n",
    "\n",
    "  # 3. Output Layer (Vocabulary Projection)\n",
    "  outputs = layers.Dense(vocab_size)(x)\n",
    "\n",
    "  # Model Definition\n",
    "  model = keras.Model(inputs=inputs, outputs=[outputs, x]) # Logits + embeddings\n",
    "\n",
    "  # Loss function (Sparse Categorical Crossentropy)\n",
    "  loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "  # Compilation (Using Adam Optimizer)\n",
    "  model.compile(\n",
    "      optimizer=\"adam\",\n",
    "      loss=[loss_fn, None],\n",
    "  )\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for word-level language modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "'''\n",
    "Aggregates IMDb review files (positive/negative, tain/test) and shuffles\n",
    "them to avoid order bias\n",
    "'''\n",
    "directories = [\n",
    "    \"aclImdb/train/pos\",\n",
    "    \"aclImdb/train/neg\",\n",
    "    \"aclImdb/test/pos\",\n",
    "    \"aclImdb/test/neg\",\n",
    "]\n",
    "for dir in directories:\n",
    "  for f in os.listdir(dir):\n",
    "    filenames.append(os.path.join(dir, f))\n",
    "\n",
    "'''\n",
    "Ensures batches are independent and identically distributed (IID),\n",
    "critical for SGD convergence\n",
    "'''\n",
    "random.shuffle(filenames)\n",
    "\n",
    "# Text Dataset Pipeline\n",
    "'''\n",
    "Reads text files line-by-line (one review per line)\n",
    "'''\n",
    "text_ds = tf_data.TextLineDataset(filenames)\n",
    "'''\n",
    "maintains a buffer of 256 samples to randomize order\n",
    "'''\n",
    "text_ds = text_ds.shuffle(buffer_size=256)\n",
    "'''\n",
    "groups samples into batches of 128 for parallel processing\n",
    "'''\n",
    "text_ds = text_ds.batch(batch_size=256)\n",
    "\n",
    "# Text Standardization\n",
    "def custom_standardization(input_string):\n",
    "  '''\n",
    "  lowercase all text\n",
    "  reduces vocabulary size\n",
    "  '''\n",
    "  lowercased = tf_strings.lower(input_string)\n",
    "  '''\n",
    "  Remove HTML tags\n",
    "  '''\n",
    "  stripped_html = tf_strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "  return tf_strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
    "\n",
    "# Text Vectorization\n",
    "'''\n",
    "Purpose - converts taw text -> integer token sequences\n",
    "'''\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = custom_standardization,\n",
    "    \n",
    "    # Top 19,999 words (reserve 0 for padding)\n",
    "    max_tokens = vocab_size - 1,\n",
    "    \n",
    "    # Output integer token IDs\n",
    "    output_mode = \"int\",\n",
    "    \n",
    "    # Pad / trim to 81 tokens\n",
    "    output_sequence_length = maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "# Language Model Input-Target Preparation\n",
    "def prepare_lm_inputs_labels(text):\n",
    "  text = tensorflow.expand_dims(text, -1)  # Shape: [batch_size, 1]\n",
    "  tokenized_sentences = vectorize_layer(text)  # Shape: [batch_size, maxlen+1]\n",
    "  x = tokenized_sentences[:, :-1]  # Input tokens (positions 0...maxlen-1)\n",
    "  y = tokenized_sentences[:, 1:]   # Target tokens (positions 1...maxlen)\n",
    "  return x, y\n",
    "\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "text_ds = text_ds.prefetch(tf_data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a Keras Callback for generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = ops.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x, verbose=0)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "\n",
    "\n",
    "# Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "start_prompt = \"this movie is\"\n",
    "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "num_tokens_generated = 40\n",
    "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Note\n",
    "*This model was trained on Google Colab using a GPU (T4) runtime. Local execution may vary in speed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.fit(text_ds, verbose=2, epochs=25, callbacks=[text_gen_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
